---
title: "Project 2"
author: "Camryn Burley, Anna Haikl, and Joseph Keogh"
date: "12-4-19"
abstract: "On our honor as students, we have neither given nor received aid on this assignment."
output:
  pdf_dCOument: default
  html_dCOument: default
---

<<<<<<< HEAD
```{r setup, include=FALSE}
#=======
#Anna's Setup
#=======
require("knitr")
datadir <- "/Users/annahaikl/Desktop/UVA/FALL 2019/4021/DATA/Air Quality"
sourcedir <-"/Users/annahaikl/Desktop/UVA/FALL 2019/4021/CODE"
opts_knit$set(root.dir = sourcedir)
library(forecast)
library(mtsdi)
library(MTS)
```

```{r setup, include = FALSE}
# Camryn Set Up
require("knitr")
datadir <- "/Users/camrynburley/Desktop/UVA/SYS4021/Data/AirQuality"
sourcedir <- "/Users/camrynburley/Desktop/UVA/SYS4021/RCode"
opts_knit$set(root.dir = sourcedir)
library(forecast)
library(mtsdi)
library(MTS)
```

# Load Data and Impute Missing Values
```{r, warning=FALSE}
setwd(datadir)

airquality = read.csv('AirQualityUCI.csv')

# replace -200 with NA
airquality[airquality == -200] <- NA

# convert integer type to numeric
intcols = c(4,5,7,8,9,10,11,12)
for(i in 1:length(intcols)){
  airquality[,intcols[i]] <- as.numeric(airquality[,intcols[i]])
}

setwd(sourcedir)

# create new data frame with just CO and NO2
AQdata = airquality[,c(3,10)]

# impute missing air quality data
f <- ~ CO.GT. + NO2.GT.
t <- c(seq(1,dim(AQdata)[1],1))
i <- mnimput(f, AQdata, eps=1e-3, ts=TRUE, method='gam', 
             ga.control=list(formula=paste(names(AQdata)[c(1:3)],'~ns(t,2)')))

# set airquality to imputed data
AQdata <- i$filled.dataset

# aggregate to daily maxima for model building
dailyAQ <- aggregate(AQdata, by=list(as.Date(airquality[,1],"%m/%d/%Y")), FUN=max)
```

# Create Testing and Training Data
```{r}
# the index to split the data on
separationIndex <- nrow(dailyAQ)-6

# split the data
air.train <- dailyAQ[1:separationIndex-1,]
air.test <- dailyAQ[separationIndex:nrow(dailyAQ),]

# making sure no data was duplicated or lost
a <- nrow(dailyAQ)
b <- nrow(air.train)
c <- nrow(air.test)

a
b
c
```

# Data Summary
```{r}
summary(air.train)
```
The data is from March 2004 through April 2005. We are modeling the ambient daily maximum carbon monoxide (CO) and nitrogen dioxide (NO2) concentrations. 

# Univariate Time Series for CO
## Visualize the Data
```{r}
# create time series
CO.ts <- ts(air.train$CO.GT.)

# visualize raw data
plot(CO.ts)
```
There is no extremely obvious trend from the plot of the time series. There may be some seasonality, but we are unsure as of now if this fluctuation is from seasonality or if it is random.

```{r}
# periodogram
pg.CO <- spec.pgram(CO.ts,spans=9,demean=T,log='no')
```
Based on the periodogram, there are multiple possibilities for the period of a seasonal component. There are a few spikes, each of which indicate a possible frequency/period to explain the seasonality of the data. We predict that if the seasonality is significant in the model, it will be based on a complex wave. 

## Seasonality
### Finding Potential Periods
```{r}
# sort the frequencies based on influence
sorted.spec <- sort(pg.CO$spec, decreasing=T, index.return=T)

# convert to periods
sorted.omegas <- pg.CO$freq[sorted.spec$ix]
sorted.Ts <- 1/pg.CO$freq[sorted.spec$ix]

# the cutoff for influential
CO.pg.cutoff <- 5

# the top periods
print('top periods')
sorted.Ts[1:CO.pg.cutoff]

# top frequencies
## to double check that this makes sense based on periodogram
print('top frequencies')
sorted.omegas[1:CO.pg.cutoff]

# visual
CO.pg.box <- boxplot(sorted.Ts[1:CO.pg.cutoff], main="Period Boxplot")

# the average influential period
print('mean of top periods')
CO.pg.box.mean <- CO.pg.box$stats[3]
print(CO.pg.box.mean)

# plot top periods
plot(sorted.Ts[1:CO.pg.cutoff], main = "Top Periods")
```
We found the frequencies of the largest spikes in the periodogram graph and converted them to periods by taking 1/the frequency. The five largest spikes correspond to periods of 384, 192, 128, 96, and 76.8 days. We considered these our "top" choices for periods to explain seasonality.  
We are concerned that the reason we are seeing a period of 384 days is because this is close to the length of the data set and not because there is actual correlation between data values collected 384 days apart. We will investigate whether or not to use this period in the following sections.

### Create Model with Potential Periods
To begin, we made a model without the 384 day period but that included the other four periods.
```{r}
# assign potential periods to variables
CO.p1 <- sorted.Ts[1]
CO.p2 <- sorted.Ts[2]
CO.p3 <- sorted.Ts[3]
CO.p4 <- sorted.Ts[4]
CO.p5 <- sorted.Ts[5]

# CO.p1
# CO.p2
# CO.p3
# CO.p4
# CO.p5

# create time variable
time.CO<-c(1:length(CO.ts))

# model
CO.lm.top4 <- lm(CO.ts ~ sin(2*pi*time.CO/CO.p2) + 
                         cos(2*pi*time.CO/CO.p2) +
                         sin(2*pi*time.CO/CO.p3) + 
                         cos(2*pi*time.CO/CO.p3) +
                         sin(2*pi*time.CO/CO.p4) + 
                         cos(2*pi*time.CO/CO.p4)+
                         sin(2*pi*time.CO/CO.p5) + 
                         cos(2*pi*time.CO/CO.p5))

# model summary
summary(CO.lm.top4)
```
This model will be compared to a model that only uses the first "important" period (i.e. the period associated with the largest spike in the periodogram, with the exception of the one corresponding to 384 days).

### Compare Larger Model with Model with Only First Important Period
```{r}
# actual model
CO.lm.top1 <- lm(CO.ts ~ sin(2*pi*time.CO/CO.p2) + cos(2*pi*time.CO/CO.p2))

# model summary
summary(CO.lm.top1)

# anova
anova(CO.lm.top1, CO.lm.top4)
```
The p-value of the partial F-test is 0.0024, which is significant at the 0.05 level. We reject the null hypothesis, which means that the larger model contains at least one coefficient, not shared with the smaller model, that is significant. The larger model is better at explaining variability.

### Create Model with All Identified Periods
```{r}
# actual model
CO.lm.top5 <- lm(CO.ts ~ sin(2*pi*time.CO/CO.p1) + 
                         cos(2*pi*time.CO/CO.p1) +
                         sin(2*pi*time.CO/CO.p2) + 
                         cos(2*pi*time.CO/CO.p2) +
                         sin(2*pi*time.CO/CO.p3) + 
                         cos(2*pi*time.CO/CO.p3) +
                         sin(2*pi*time.CO/CO.p4) + 
                         cos(2*pi*time.CO/CO.p4)+
                         sin(2*pi*time.CO/CO.p5) + 
                         cos(2*pi*time.CO/CO.p5))

# model summary
summary(CO.lm.top5)

# compare with smaller model
anova(CO.lm.top4, CO.lm.top5)
```
We created the model that used all five periods identified from the periodogram. We compared it to the model without the 384-day period using a partial F test. The test is significant at the 0.05 level, which means there is significant evidence to use the larger model. We will use the model with all five periods to explain seasonality in further analysis.

### Visual Inspection of Model
```{r}
plot(CO.ts)
lines(CO.lm.top5$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(CO.lm.top5$fitted.values, col = "red")

plot(CO.ts, xlim=c(100,200))
lines(CO.lm.top5$fitted.values, col = "red")
```
We plotted the fitted values of the model in red over the graph of the time series data. We plotted the entire time series and also zoomed in to see t = [0, 100] and t = [100, 200]. The model follows the general trend of the data, but there are no small fluctuations, of which the data has many. We may need to add a term with a smaller period in order to capture these smaller fluctuations.

### Explore Adding Higher Frequency Components
```{r}
# visual for frequency
plot(sorted.omegas, xlim=c(0,10))

# visual for periods
plot(sorted.Ts, xlim=c(0,30))

# look at the 6th through 10th most influencial periods
next.low.period <- boxplot(sorted.Ts[6:10])$stats[3]
next.low.period
```
The average of the periods associated with the next five highest spikes of the periodogram is about 7 days. A period of around 7 days makes sense to us, as CO production could be vary weekly from people working and commuting.

### Add New Period to Model
We made a new model with all five periods in our previous model and the addition of the new period around 7 days.
```{r}
CO.p6 <- next.low.period

# model
CO.lm.top6 <- lm(CO.ts ~ sin(2*pi*time.CO/CO.p1) + 
                         cos(2*pi*time.CO/CO.p1) +
                         sin(2*pi*time.CO/CO.p2) + 
                         cos(2*pi*time.CO/CO.p2) +
                         sin(2*pi*time.CO/CO.p3) + 
                         cos(2*pi*time.CO/CO.p3) +
                         sin(2*pi*time.CO/CO.p4) + 
                         cos(2*pi*time.CO/CO.p4) +
                         sin(2*pi*time.CO/CO.p5) + 
                         cos(2*pi*time.CO/CO.p5) +
                         sin(2*pi*time.CO/CO.p6) + 
                         cos(2*pi*time.CO/CO.p6))

# model summary
summary(CO.lm.top6)

# visual inspection
plot(CO.ts)
lines(CO.lm.top6$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(CO.lm.top6$fitted.values, col = "red")

plot(CO.ts, xlim=c(100,200))
lines(CO.lm.top6$fitted.values, col = "red")
```
Upon visually inspecting this new model, we see smaller fluctuations, which we wanted. However, we do see that the peaks of the smaller waves do not consistently match up with where the data has a peak, based on the zoomed in graphs.  
After determining if there is a significant trend, we will compare the potential trend and seasonality models with five and 6 periods, to determine which one will comprise our final linear model.

## Trend
### Model Time Series Based on Time
```{r}
# model
CO.lm.trend <- lm(CO.ts ~ time.CO)

# summary analysis
summary(CO.lm.trend)
```
The p-value is 0.00097 so there is  a significant trend in the data.

### Visualize Trend
```{r}
plot(CO.ts)
abline(CO.lm.trend, col='red')
```

### Model Comparison: Trend and Seasonality Together
```{r}
# model with trend and 5 periods
CO.seasonal5.trend <- lm(CO.ts ~ CO.lm.trend$effects + CO.lm.top5$effects)

# model with trend and 6 periods 
CO.seasonal6.trend <- lm(CO.ts ~ CO.lm.trend$effects + CO.lm.top6$effects)
```

#### Visual Comparison
```{r}

model1 <- CO.seasonal5.trend
plot(CO.ts)
lines(model1$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(model1$fitted.values, col = "red")

model2 <- CO.seasonal6.trend
plot(CO.ts)
lines(model2$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(model2$fitted.values, col = "red")
```
Based on visual inspection, both models are pretty good, especially at modeling the data at the beginning of the time series. After that, while the peaks are not of the same magnitude as those of the data, they occur at relatively the same x-value in both plots. The differences in the models are slight, which makes it difficult to determine the best one by eye. 

#### Comparison Using Metrics
```{r}
# Adjusted R^2: model with trend and 5 periods
summary(CO.seasonal5.trend)$adj.r.squared

# AIC: model with trend and 5 periods
AIC(CO.seasonal5.trend)

# Adjusted R^2: model with trend and 6 periods
summary(CO.seasonal6.trend)$adj.r.squared

# AIC: model with trend and 6 periods
AIC(CO.seasonal6.trend)
```
The adjusted R^2 of the model that includes the trend and 5 periods is 0.258. The adjusted R^2 of the model that includes the trend and 6 periods is 0.259. Based on adjusted R^2, we would select the model with 6 periods, though they are very close based on this metric and neither value is particularly close to 1.  
The AIC of the model with 5 periods is 1474.224. The AIC of the model with 6 periods is 1473.733. Based on AIC, we would choose the model with 6 periods, because it has the smaller AIC value. Again, the values are very similar, though.

#### Diagnostics
**Residuals vs. Fitted**
```{r}
plot(CO.seasonal5.trend, which = 1)
plot(CO.seasonal6.trend, which = 1)
```
The mean of the residuals is not 0 and the variance is not constant for either model. The relationship evident in the plots indicates a lack of fit.

\pagebreak
**QQ Plot**
```{r}
plot(CO.seasonal5.trend, which = 2)
plot(CO.seasonal6.trend, which = 2)
```
The residuals are approximately normal for both models, though the tails show some deviation from normality.

\pagebreak
**Scale-Location Plot**
```{r}
plot(CO.seasonal5.trend, which = 3)
plot(CO.seasonal6.trend, which = 3)
```
The data appears to be clustered together in both plots, and the mean is not centered at 0.

\pagebreak
**Residuals vs. Leverage**
```{r}
plot(CO.seasonal5.trend, which = 5)
plot(CO.seasonal6.trend, which = 5)
```
In both plots, there is one point, which is the same data point, with a Cook's distance larger than 1. There is also one shared point that has a Cook's distance of approximately 0.5.

\pagebreak
```{r}
# plot Cook's distances
plot(CO.seasonal5.trend,labels.id = NULL, which = 4)
plot(CO.seasonal6.trend,labels.id = NULL, which = 4)
```
These plots show that it is the same two points for each model with high Cook's distances.

Both models perform very similarly in the diagnostics.

### Choose Seasonal Model
```{r}
CO.lm.seasonal <- CO.lm.top6
```
We have chosen the model with the trend and top six identified periods to explain seasonality, as it performed better than the model with trend and five periods in Adjusted R^2 and AIC. Since the diagnostics for both models were similar, these criteria were used to make the decision.

## Model Residuals
### ACF and PACF
```{r}
# ACF and PACF
acf(CO.ts)
pacf(CO.ts)
```
The ACF is approximately sinusoidal. The PACF does not cut off after a certain number of lags and has some sinusoidal behavior. We predict that there will be autoregressive and moving average terms in the model of the residuals.  
Based on the PACF, potential values for p for the autoregressive portion of the model are 4 and 8.  
Based on the ACF, potential values for q for the moving average portion of the model are 4 and 11.  
Additionally, we are unsure of the stationarity of the data, as we know there is a trend and the ACF appears to have some linear decay at the very beginning. This is a key assumption that must be met, so we will test models using the time series itself (d = 0) and the first differences (d = 1).

### ARIMA Models
We tested the possible combinations of our p, d, and q values.
```{r}
# get residuals
e.ts.CO <-ts(CO.seasonal.trend$residuals)

# 4, 0, 4
CO.arima404 <- arima(e.ts.CO, order = c(4,0,4), include.mean = FALSE)
summary(CO.arima404)
# AIC = 1389.47

# 4, 0, 11
CO.arima4011 <- arima(e.ts.CO, order = c(4,0,11), include.mean = FALSE)
summary(CO.arima4011)
# AIC = 1387.91

# 4, 1, 4
CO.arima414 <- arima(e.ts.CO, order = c(4,1,4), include.mean = FALSE)
summary(CO.arima414)
# AIC = 1394.41

# 4, 1, 11
CO.arima4111 <- arima(e.ts.CO, order = c(4,1,11), include.mean = FALSE)
summary(CO.arima4111)
# AIC = 1393.15

# 8, 0, 4
CO.arima804 <- arima(e.ts.CO, order = c(8,0,4), include.mean = FALSE)
summary(CO.arima804)
# AIC = 1373.65

# 8, 0, 11
CO.arima8011 <- arima(e.ts.CO, order = c(8,0,11), include.mean = FALSE)
summary(CO.arima8011)
# AIC = 1369.56 
# NaNs produced

# 8, 1, 4
# CO.arima814 <- arima(e.ts.CO, order = c(8,1,4), include.mean = FALSE)
# summary(CO.arima814)
# produced errors, so was not included 

# 8, 1, 11
CO.arima8111 <- arima(e.ts.CO, order = c(8,1,11), include.mean = FALSE)
summary(CO.arima8111)
# AIC = 1375.45
# NaNs produced
```
Based on AIC, the two best models are the ARIMA(4,0,11) and ARIMA(8,0,4) models.

After testing the possible values of p, d, and q that we identified, we also used the auto.arima function to generate another model.
```{r}
# auto.arima model
CO.residuals.auto <- auto.arima(e.ts.CO)

# summary
summary(CO.residuals.auto)
```
The ARIMA model is a 5,1,4 model. This means that there are autoregressive and moving average terms, where p = 5 and q = 4. Also, d = 1, meaning that the first differences were used to ensure stationarity, which is a key assumption to modeling the residuals. The AIC for this model is 1396.76.

### Diagnostics
```{r}
tsdiag(CO.arima4011, gof.lag = 20)
tsdiag(CO.arima804, gof.lag = 20)
tsdiag(CO.residuals.auto, gof.lag = 20)
```
The model that performs the best in diagnostics is the ARIMA(4,0,11) model. It has the most points with high p-values in the Ljung-Box statistic plot. Both of the other models see their p-values decrease as the lag increases.

### Choose Model for Residuals
```{r}
CO.residuals <- CO.arima4011
```

## Final Model
The final model includes a trend and seasonality, using 6 different periods (384, 192, 128, 96, 76.8, and 7.11 days). The ARIMA model of the residuals is a 4,0,11 model.

## Diagnostics
The residuals of the trend and seasonality linear model appear relatively normal, from the QQ plot; however, problems remain with the residuals vs. fitted and scale-location plots, indicating violation of assumptions and lack of fit. There are also two points with high Cook's distances. In the future, models should address these problems.
The diagnostic plot for the model of the residuals does not have problems. The p-value for the Ljung-Box stastic is above the dashed line for all of the points and approaches 1 for many of the points. The model of the residuals is adequate for up to 20 lags.


# Univariate Time Series for NO2
## Visualize the Data
```{r}
# create time series
NO.ts <- ts(air.train$NO2.GT.)

# visualize raw data
plot(NO.ts)

# periodogram
pg.NO <- spec.pgram(NO.ts,spans=9,demean=T,log='no')

# acf and pacf
acf(NO.ts)
pacf(NO.ts)
```

No obvious trend or seasonality based on raw data

based on periodogram there may be some seasonlity, but it is going to be based on complex waves

Determine if there is seasonality in the data
```{r}
# sort the frequencies based on influence
sorted.spec.NO <- sort(pg.NO$spec, decreasing=T, index.return=T)

# convert to periods
sorted.omegas.NO <- pg.NO$freq[sorted.spec.NO$ix]
sorted.Ts.NO <- 1/pg.NO$freq[sorted.spec.NO$ix]

plot(sorted.Ts.NO, xlim = c(1,20))

# the cutoff for influencial
NO.pg.cutoff <- 15


# the top periods
print('top periods')
sorted.Ts.NO[1:NO.pg.cutoff]

# top frequencies
## to double check that this makes sense based on periodogram
print('top frequencies')
sorted.omegas.NO[1:NO.pg.cutoff]

# visual
NO.pg.box <- boxplot(sorted.Ts.NO[1:NO.pg.cutoff], main="Period Boxplot")

# the average influencial period
print('mean of top periods')
NO.pg.box.mean <- NO.pg.box$stats[3]
print(NO.pg.box.mean)
```

The average influencial period can be interpreted as on average the seasons have a period of 48 days

The top 15 periods range from 6.62 to 384 days

The top 15 frequencies range from 0.0026 to 0.15
    
Assign top periods to variables
```{r}
NO.p1 <- sorted.Ts.NO[1]
NO.p2 <- sorted.Ts.NO[2]
NO.p3 <- sorted.Ts.NO[3]
NO.p4 <- sorted.Ts.NO[4]
NO.p5 <- sorted.Ts.NO[5]
NO.p6 <- sorted.Ts.NO[6]
NO.p7 <- sorted.Ts.NO[7]
NO.p8 <- sorted.Ts.NO[8]
NO.p9 <- sorted.Ts.NO[9]
NO.p10 <- sorted.Ts.NO[10]
NO.p11 <- sorted.Ts.NO[11]
NO.p12 <- sorted.Ts.NO[12]
NO.p13 <- sorted.Ts.NO[13]
NO.p14 <- sorted.Ts.NO[14]
NO.p15 <- sorted.Ts.NO[15]

NO.pavg1 <- mean(c(sorted.Ts.NO[8],sorted.Ts.NO[9], sorted.Ts.NO[10], sorted.Ts.NO[11]))
NO.pavg2 <- mean(c(sorted.Ts.NO[12],sorted.Ts.NO[13], sorted.Ts.NO[14], sorted.Ts.NO[15]))
```

NO Model for top 3 Periods
```{r}
# create time variable
time.NO<-c(1:length(NO.ts))

# actual model
NO.lm.top3 <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3)) 

# model summary
summary(NO.lm.top3)
```
NO Model for top 5 Periods
```{r}
# actual model
NO.lm.top5 <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5)) 

# model summary
summary(NO.lm.top5)
```
NO Model for top 10 Periods
```{r}
# actual model
NO.lm.top10 <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) + 
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.p8) + 
                         cos(2*pi*time.NO/NO.p8) +
                         sin(2*pi*time.NO/NO.p9) + 
                         cos(2*pi*time.NO/NO.p9) +
                         sin(2*pi*time.NO/NO.p10) + 
                         cos(2*pi*time.NO/NO.p10)) 

# model summary
summary(NO.lm.top10)
```
NO Model for top 15 Periods
```{r}
# actual model
NO.lm.top15 <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) + 
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.p8) + 
                         cos(2*pi*time.NO/NO.p8) +
                         sin(2*pi*time.NO/NO.p9) + 
                         cos(2*pi*time.NO/NO.p9) +
                         sin(2*pi*time.NO/NO.p10) + 
                         cos(2*pi*time.NO/NO.p10) +
                         sin(2*pi*time.NO/NO.p11) + 
                         cos(2*pi*time.NO/NO.p11) + 
                         sin(2*pi*time.NO/NO.p12) + 
                         cos(2*pi*time.NO/NO.p12) +
                         sin(2*pi*time.NO/NO.p13) + 
                         cos(2*pi*time.NO/NO.p13) +
                         sin(2*pi*time.NO/NO.p14) + 
                         cos(2*pi*time.NO/NO.p14) +
                         sin(2*pi*time.NO/NO.p15) + 
                         cos(2*pi*time.NO/NO.p15)) 

# model summary
summary(NO.lm.top15)
```
NO Model with Averaged Periods 
```{r}
NO.lm.combined <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) +
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.pavg1) + 
                         cos(2*pi*time.NO/NO.pavg1) +
                         sin(2*pi*time.NO/NO.pavg2) + 
                         cos(2*pi*time.NO/NO.pavg2)) 

# model summary
summary(NO.lm.combined)
```

Model Comparison
```{r}
anova(NO.lm.top3, NO.lm.top5)
#small p-value suggests that the larger model performs better than the smaller model 

anova(NO.lm.top5, NO.lm.top10)
#small p-value suggests that the larger model performs better than the smaller model 

anova(NO.lm.top10, NO.lm.top15)
#large p-value suggests that the smaller model performs better 

anova(NO.lm.top10, NO.lm.combined)
#large p-value suggests that the smaller model performs better 

anova(NO.lm.combined, NO.lm.top5)
#small p-value suggests that the larger model performs better
```

<<<<<<< HEAD
The partial-F tests suggest that the largest model, the one with 10 periods included, is the best predictor of the time series 

Visually inspect step model 
```{r}
plot(NO.ts)
lines(NO.lm.top3$fitted.values, col = "red")
lines(NO.lm.top5$fitted.values, col = "blue")
lines(NO.lm.top10$fitted.values, col = "green")
lines(NO.lm.combined$fitted.values, col = "orange")

legend(0, 350, legend = c("Top 3 Periods", "Top 5 Periods", "Top 10 Periods", "Combined Periods"), col = c("red", "blue", "green", "orange"), lwd = 1)
```
The graph suggests that the model that contains the combined periods is the best predictor

Visual Inspection of the Combined Periods 
```{r}
plot(NO.ts)
lines(NO.lm.combined$fitted.values, col = "blue")
```
<!-- ======= -->
<!-- # Multivariate Time Series -->
<!-- ## Seasonality -->
<!-- ## Trend -->
<!-- ## Autoregressive and Moving Average Components -->
<!-- ### Metrics -->
<!-- compare models based on adjusted R2 and/or AIC -->
<!-- ### Diagnostics -->
<!-- diagnostics of linear models of trends + seasonality and ARIMA models of residuals -->
<!-- ### Forecasting -->
<!-- compare models based on MSE -->

<!-- ## Final Model -->

<!-- ## Diagnostics -->
<!-- what problems remain in the diagnostics of the selected model? -->

Determine if there is a trend in the data 
```{r}
# the actual model
NO.lm.trend <- lm(NO.ts ~ time.NO)

# summary analysis
summary(NO.lm.trend)

plot(time.NO, NO.ts, type = "l")
abline(NO.lm.trend, col = "red")
```
This shows that there is a trend in the data 

Make a combined model of seasonality and trend 
```{r}
NO.trendseason <- lm(NO.ts ~ time.NO + sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) +
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.pavg1) + 
                         cos(2*pi*time.NO/NO.pavg1) +
                         sin(2*pi*time.NO/NO.pavg2) + 
                         cos(2*pi*time.NO/NO.pavg2))

summary(NO.trendseason)

plot(NO.ts)
lines(NO.trendseason$fitted.values, col = "blue")
lines(NO.lm.combined$fitted.values, col = "red")

anova(NO.trendseason, NO.lm.combined)

```
Decided to proceed using the model that incorporates trend and seasonality 

ARIMA Model 
```{r}
e.ts.NO <- NO.trendseason$residuals
acf(e.ts.NO)
#Because there is no linear decay in the ACF then the residuals are stationary
#cuts off after 2 lags
pacf(e.ts.NO)
#cuts off after 1 or 5 lags 
```
Make ARIMA models 
```{r}
NO.arima201 <- arima(e.ts.NO, order = c(2,0,1), include.mean = FALSE)
NO.arima205 <- arima(e.ts.NO, order = c(2,0,5), include.mean = FALSE)
NO.arima.auto <- auto.arima(e.ts.NO, approximation = FALSE)
```

Compare ARIMA Mdoels 
```{r}
#Compare Visually - Not sure if this is a legit way of doing this
plot(e.ts.NO, type = "line")
lines(NO.arima201$residuals,col = "red")
plot(e.ts.NO, type = "line")
lines(NO.arima205$residuals,col = "blue")
plot(e.ts.NO, type = "line")
lines(NO.arima.auto$residuals,col = "green")

#AIC
AIC(NO.arima201)
AIC(NO.arima205)
AIC(NO.arima.auto)

#BIC
BIC(NO.arima201)
BIC(NO.arima205)
BIC(NO.arima.auto)

#Diagnostics
tsdiag(NO.arima201, gof.lag = 20)
tsdiag(NO.arima205, gof.lag = 20)
tsdiag(NO.arima.auto, gof.lag = 20)

```


## Model Assessment 
### Metrics
compare models based on adjusted R2 and/or AIC
### Diagnostics
diagnostics of linear models of trends + seasonality and ARIMA models of residuals






=======
# Simulation from Univariate CO Model
```{r}
# CO univariate model = model of data + model of residuals
CO.uni <- CO.final + CO.resi.final # idk that this is the way to do this

# simulate 1 year of daily maximum CO concentrations
### fix this based on actual ar and ma terms
CO.uni.sim <- arima.sim(n = 365*1, list(ar = c(CO.uni$coef[1], CO.uni$coef[2]), ma = c(CO.uni$coef[3])), sd = sqrt(CO.uni$sigma2))

# make time series
CO.uni.sim.ts <- ts(CO.uni.sim)
```

## Visualization
```{r}
# plot simulated values with observations
plot(CO.ts, main = "Simulation from Univariate CO Model", col = "black")
lines(CO.uni.sim.ts, col = "red")
legend(0.6, 0.57, legend = c("Time Series Data", "Simulation"), col = c("black", "red"))
```
how does simulation compare visually with time series data?

## Trend
```{r}
# linear model for simulation
CO.sim.lm <- lm()
```
compare coefficient estimates

## Seasonality
```{r}
# compare periodogram of observations and periodogram of simulation
par(mfrow = c(1,2))
pg.CO
pg.CO.uni.sim <- spec.pgram(CO.uni.sim.ts, spans = 9, demean = T, log = 'no')
par(mfrow = c(1,1))
```
compare periodograms (get periods?)

## Mean and Variance
```{r}
# mean of observations
mean(CO.ts)

# mean of simulation
mean(CO.uni.sim.ts)

# variance of observations
var(CO.ts)

# variance of simulation
var(CO.uni.sim.ts)
```
compare mean and variance

## Auto-Correlation
```{r}
# ACF of observations
acf(CO.ts)

# ACF of simulation
acf(CO.uni.sim.ts)
```
compare ACF

```{r}
# PACF of observations
pacf(CO.ts)

# PACF of simulation
pacf(CO.uni.sim.ts)
```
compare PACF

## Cross-Correlation
```{r}
# cross-correlation of observations
cor(air.train$CO.GT.)

# cross-correlation of simulation
cor(CO.uni.sim)
```
compare cross-correlation

# Forecasting
compare univariate and multivariate models based on MSE and visually

# Simulation from Univariate NO2 Model
## Visualization
## Trend
## Seasonality
## Mean and Variance
## Auto-Correlation
## Cross-Correlation


# Simulation from Multivariate Model
## Visualization
## Trend
## Seasonality
## Mean and Variance
## Auto-Correlation
## Cross-Correlation

