---
title: "Project 2"
author: "Camryn Burley, Anna Haikl, and Joseph Keogh"
date: "12-4-19"
abstract: "On our honor as students, we have neither given nor received aid on this assignment."
output:
  pdf_dCOument: default
  html_dCOument: default
---

```{r setup, include=FALSE}
#=======
#Anna's Setup
#=======
require("knitr")
datadir <- "/Users/annahaikl/Desktop/UVA/FALL 2019/4021/DATA/Air Quality"
sourcedir <-"/Users/annahaikl/Desktop/UVA/FALL 2019/4021/CODE"
opts_knit$set(root.dir = sourcedir)
library(forecast)
library(mtsdi)
library(MTS)
```

```{r setup, include = FALSE}
# Camryn Set Up
require("knitr")
datadir <- "/Users/camrynburley/Desktop/UVA/SYS4021/Data/AirQuality"
sourcedir <- "/Users/camrynburley/Desktop/UVA/SYS4021/RCode"
opts_knit$set(root.dir = sourcedir)
library(forecast)
library(mtsdi)
library(MTS)
```


```{r}
# Joey Set Up
require("knitr")
datadir <- "C:/Users/JK/Google Drive/Acedemics/College/Fourth Year/Semester 7/Linear Statistical Models/Data/AirQualityData"
sourcedir <-"C:/Users/JK/Google Drive/Acedemics/College/Fourth Year/Semester 7/Linear Statistical Models/Code"
opts_knit$set(root.dir = sourcedir)
library(forecast)
library(mtsdi)
library(MTS)
```


# Load Data and Impute Missing Values
```{r, warning=FALSE}
setwd(datadir)

airquality = read.csv('AirQualityUCI.csv')

# replace -200 with NA
airquality[airquality == -200] <- NA

# convert integer type to numeric
intcols = c(4,5,7,8,9,10,11,12)
for(i in 1:length(intcols)){
  airquality[,intcols[i]] <- as.numeric(airquality[,intcols[i]])
}

setwd(sourcedir)

# create new data frame with just CO and NO2
AQdata = airquality[,c(3,10)]

# impute missing air quality data
f <- ~ CO.GT. + NO2.GT.
t <- c(seq(1,dim(AQdata)[1],1))
i <- mnimput(f, AQdata, eps=1e-3, ts=TRUE, method='gam', 
             ga.control=list(formula=paste(names(AQdata)[c(1:3)],'~ns(t,2)')))

# set airquality to imputed data
AQdata <- i$filled.dataset

# aggregate to daily maxima for model building
dailyAQ <- aggregate(AQdata, by=list(as.Date(airquality[,1],"%m/%d/%Y")), FUN=max)
```

# Create Testing and Training Data
```{r}
# the index to split the data on
separationIndex <- nrow(dailyAQ)-6

# split the data
air.train <- dailyAQ[1:separationIndex-1,]
air.test <- dailyAQ[separationIndex:nrow(dailyAQ),]

# making sure no data was duplicated or lost
a <- nrow(dailyAQ)
b <- nrow(air.train)
c <- nrow(air.test)

a
b
c
```

# Data Summary
```{r}
summary(air.train)
```
The data is from March 2004 through April 2005. We are modeling the ambient daily maximum carbon monoxide (CO) and nitrogen dioxide (NO2) concentrations. 

# Univariate Time Series for CO
## Visualize the Data
```{r}
# create time series
CO.ts <- ts(air.train$CO.GT.)

# visualize raw data
plot(CO.ts)
```
There is no extremely obvious trend from the plot of the time series. There may be some seasonality, but we are unsure as of now if this fluctuation is from seasonality or if it is random.

```{r}
# periodogram
pg.CO <- spec.pgram(CO.ts,spans=9,demean=T,log='no')
```
Based on the periodogram, there are multiple possibilities for the period of a seasonal component. There are a few spikes, each of which indicate a possible frequency/period to explain the seasonality of the data. We predict that if the seasonality is significant in the model, it will be based on a complex wave. 

## Seasonality
### Finding Potential Periods
```{r}
# sort the frequencies based on influence
sorted.spec <- sort(pg.CO$spec, decreasing=T, index.return=T)

# convert to periods
sorted.omegas <- pg.CO$freq[sorted.spec$ix]
sorted.Ts <- 1/pg.CO$freq[sorted.spec$ix]

# the cutoff for influential
CO.pg.cutoff <- 5

# the top periods
print('top periods')
sorted.Ts[1:CO.pg.cutoff]

# top frequencies
## to double check that this makes sense based on periodogram
print('top frequencies')
sorted.omegas[1:CO.pg.cutoff]

# visual
CO.pg.box <- boxplot(sorted.Ts[1:CO.pg.cutoff], main="Period Boxplot")

# the average influential period
print('mean of top periods')
CO.pg.box.mean <- CO.pg.box$stats[3]
print(CO.pg.box.mean)

# plot top periods
plot(sorted.Ts[1:CO.pg.cutoff], main = "Top Periods")
```
We found the frequencies of the largest spikes in the periodogram graph and converted them to periods by taking 1/the frequency. The five largest spikes correspond to periods of 384, 192, 128, 96, and 76.8 days. We considered these our "top" choices for periods to explain seasonality.  
We are concerned that the reason we are seeing a period of 384 days is because this is close to the length of the data set and not because there is actual correlation between data values collected 384 days apart. We will investigate whether or not to use this period in the following sections.

### Create Model with Potential Periods
To begin, we made a model without the 384 day period but that included the other four periods.
```{r}
# assign potential periods to variables
CO.p1 <- sorted.Ts[1]
CO.p2 <- sorted.Ts[2]
CO.p3 <- sorted.Ts[3]
CO.p4 <- sorted.Ts[4]
CO.p5 <- sorted.Ts[5]

# CO.p1
# CO.p2
# CO.p3
# CO.p4
# CO.p5

# create time variable
time.CO<-c(1:length(CO.ts))

# model
CO.lm.top4 <- lm(CO.ts ~ sin(2*pi*time.CO/CO.p2) + 
                         cos(2*pi*time.CO/CO.p2) +
                         sin(2*pi*time.CO/CO.p3) + 
                         cos(2*pi*time.CO/CO.p3) +
                         sin(2*pi*time.CO/CO.p4) + 
                         cos(2*pi*time.CO/CO.p4)+
                         sin(2*pi*time.CO/CO.p5) + 
                         cos(2*pi*time.CO/CO.p5))

# model summary
summary(CO.lm.top4)
```
This model will be compared to a model that only uses the first "important" period (i.e. the period associated with the largest spike in the periodogram, with the exception of the one corresponding to 384 days).

### Compare Larger Model with Model with Only First Important Period
```{r}
# actual model
CO.lm.top1 <- lm(CO.ts ~ sin(2*pi*time.CO/CO.p2) + cos(2*pi*time.CO/CO.p2))

# model summary
summary(CO.lm.top1)

# anova
anova(CO.lm.top1, CO.lm.top4)
```
The p-value of the partial F-test is 0.0024, which is significant at the 0.05 level. We reject the null hypothesis, which means that the larger model contains at least one coefficient, not shared with the smaller model, that is significant. The larger model is better at explaining variability.

### Create Model with All Identified Periods
```{r}
# actual model
CO.lm.top5 <- lm(CO.ts ~ sin(2*pi*time.CO/CO.p1) + 
                         cos(2*pi*time.CO/CO.p1) +
                         sin(2*pi*time.CO/CO.p2) + 
                         cos(2*pi*time.CO/CO.p2) +
                         sin(2*pi*time.CO/CO.p3) + 
                         cos(2*pi*time.CO/CO.p3) +
                         sin(2*pi*time.CO/CO.p4) + 
                         cos(2*pi*time.CO/CO.p4)+
                         sin(2*pi*time.CO/CO.p5) + 
                         cos(2*pi*time.CO/CO.p5))

# model summary
summary(CO.lm.top5)

# compare with smaller model
anova(CO.lm.top4, CO.lm.top5)
```
We created the model that used all five periods identified from the periodogram. We compared it to the model without the 384-day period using a partial F test. The test is significant at the 0.05 level, which means there is significant evidence to use the larger model. We will use the model with all five periods to explain seasonality in further analysis.

### Visual Inspection of Model
```{r}
plot(CO.ts)
lines(CO.lm.top5$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(CO.lm.top5$fitted.values, col = "red")

plot(CO.ts, xlim=c(100,200))
lines(CO.lm.top5$fitted.values, col = "red")
```
We plotted the fitted values of the model in red over the graph of the time series data. We plotted the entire time series and also zoomed in to see t = [0, 100] and t = [100, 200]. The model follows the general trend of the data, but there are no small fluctuations, of which the data has many. We may need to add a term with a smaller period in order to capture these smaller fluctuations.

### Explore Adding Higher Frequency Components
```{r}
# visual for frequency
plot(sorted.omegas, xlim=c(0,10))

# visual for periods
plot(sorted.Ts, xlim=c(0,30))

# look at the 6th through 10th most influencial periods
next.low.period <- boxplot(sorted.Ts[6:10])$stats[3]
next.low.period
```
The average of the periods associated with the next five highest spikes of the periodogram is about 7 days. A period of around 7 days makes sense to us, as CO production could be vary weekly from people working and commuting.

### Add New Period to Model
We made a new model with all five periods in our previous model and the addition of the new period around 7 days.
```{r}
CO.p6 <- next.low.period

# model
CO.lm.top6 <- lm(CO.ts ~ sin(2*pi*time.CO/CO.p1) + 
                         cos(2*pi*time.CO/CO.p1) +
                         sin(2*pi*time.CO/CO.p2) + 
                         cos(2*pi*time.CO/CO.p2) +
                         sin(2*pi*time.CO/CO.p3) + 
                         cos(2*pi*time.CO/CO.p3) +
                         sin(2*pi*time.CO/CO.p4) + 
                         cos(2*pi*time.CO/CO.p4) +
                         sin(2*pi*time.CO/CO.p5) + 
                         cos(2*pi*time.CO/CO.p5) +
                         sin(2*pi*time.CO/CO.p6) + 
                         cos(2*pi*time.CO/CO.p6))

# model summary
summary(CO.lm.top6)

# visual inspection
plot(CO.ts)
lines(CO.lm.top6$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(CO.lm.top6$fitted.values, col = "red")

plot(CO.ts, xlim=c(100,200))
lines(CO.lm.top6$fitted.values, col = "red")
```
Upon visually inspecting this new model, we see smaller fluctuations, which we wanted. However, we do see that the peaks of the smaller waves do not consistently match up with where the data has a peak, based on the zoomed in graphs.  
After determining if there is a significant trend, we will compare the potential trend and seasonality models with five and 6 periods to determine which one will comprise our final linear model.

## Trend
### Model Time Series Based on Time
```{r}
# model
CO.lm.trend <- lm(CO.ts ~ time.CO)

# summary analysis
summary(CO.lm.trend)
```
The p-value is 0.00097, so there is a significant trend in the data.

### Visualize Trend
```{r}
plot(CO.ts)
abline(CO.lm.trend, col='red')
```

### Model Comparison: Trend and Seasonality Together
```{r}
# model with trend and 5 periods
CO.seasonal5.trend <- lm(CO.ts ~ time.CO + sin(2*pi*time.CO/CO.p1) + 
                         cos(2*pi*time.CO/CO.p1) +
                         sin(2*pi*time.CO/CO.p2) + 
                         cos(2*pi*time.CO/CO.p2) +
                         sin(2*pi*time.CO/CO.p3) + 
                         cos(2*pi*time.CO/CO.p3) +
                         sin(2*pi*time.CO/CO.p4) + 
                         cos(2*pi*time.CO/CO.p4)+
                         sin(2*pi*time.CO/CO.p5) + 
                         cos(2*pi*time.CO/CO.p5))

# model with trend and 6 periods 
CO.seasonal6.trend <- lm(CO.ts ~ time.CO + sin(2*pi*time.CO/CO.p1) + 
                         cos(2*pi*time.CO/CO.p1) +
                         sin(2*pi*time.CO/CO.p2) + 
                         cos(2*pi*time.CO/CO.p2) +
                         sin(2*pi*time.CO/CO.p3) + 
                         cos(2*pi*time.CO/CO.p3) +
                         sin(2*pi*time.CO/CO.p4) + 
                         cos(2*pi*time.CO/CO.p4) +
                         sin(2*pi*time.CO/CO.p5) + 
                         cos(2*pi*time.CO/CO.p5) +
                         sin(2*pi*time.CO/CO.p6) + 
                         cos(2*pi*time.CO/CO.p6))

anova(CO.seasonal5.trend, CO.seasonal6.trend)
```
The p-value is large (not significant at the 0.05 level), which means that the smaller model is preferred.

#### Visual Comparison
```{r}
model1 <- CO.seasonal5.trend
plot(CO.ts)
lines(model1$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(model1$fitted.values, col = "red")

model2 <- CO.seasonal6.trend
plot(CO.ts)
lines(model2$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(model2$fitted.values, col = "red")
```
Based on visual inspection, both models are pretty good, especially at modeling the data at the beginning of the time series. After that, while the peaks are not of the same magnitude as those of the data, they occur at relatively the same x-value in both plots. The differences in the models are slight, which makes it difficult to determine the best one by eye. 

#### Comparison Using Metrics
```{r}
# Adjusted R^2: model with trend and 5 periods
summary(CO.seasonal5.trend)$adj.r.squared

# AIC: model with trend and 5 periods
AIC(CO.seasonal5.trend)

# Adjusted R^2: model with trend and 6 periods
summary(CO.seasonal6.trend)$adj.r.squared

# AIC: model with trend and 6 periods
AIC(CO.seasonal6.trend)
```
The adjusted R^2 of the model that includes the trend and 5 periods is 0.185. The adjusted R^2 of the model that includes the trend and 6 periods is 0.187. Based on adjusted R^2, we would select the model with 6 periods, though they are very close based on this metric and neither value is particularly close to 1.  
The AIC of the model with 5 periods is 1519.205. The AIC of the model with 6 periods is 1520.075. Based on AIC, we would choose the model with 5 periods, because it has the smaller AIC value. Again, the values are very similar, though.

#### Diagnostics
**Residuals vs. Fitted**
```{r}
plot(CO.seasonal5.trend, which = 1)
plot(CO.seasonal6.trend, which = 1)
```
The mean of the residuals is approximately 0 and the variance is not constant for either model. There is less variance (less spread above and below the x-axis) on the left-hand side of the plot, and the variance increases to the right. The relationship evident in the plots indicates a lack of fit.

\pagebreak
**QQ Plot**
```{r}
plot(CO.seasonal5.trend, which = 2)
plot(CO.seasonal6.trend, which = 2)
```
The residuals are approximately normal for both models, though the tails, and the lower tails especially, show some deviation from normality.

\pagebreak
**Scale-Location Plot**
```{r}
plot(CO.seasonal5.trend, which = 3)
plot(CO.seasonal6.trend, which = 3)
```
The mean is not centered at 0 and there may be a slight pattern to the scatter in both plots.

\pagebreak
**Residuals vs. Leverage**
```{r}
plot(CO.seasonal5.trend, which = 5)
plot(CO.seasonal6.trend, which = 5)
```
Neither plot has any points with Cook's distances greater than 0.5.

\pagebreak
```{r}
# plot Cook's distances
plot(CO.seasonal5.trend,labels.id = NULL, which = 4)
plot(CO.seasonal6.trend,labels.id = NULL, which = 4)
```
These plots show that all of the points have small Cook's distances for both models.

Both models perform very similarly in the diagnostics.

### testing another method
```{r}
CO.test <- lm(CO.ts ~ CO.lm.trend$effects + CO.seasonal6.trend$effects)

# plot the test
model1 <- CO.test
plot(CO.ts)
lines(model1$fitted.values, col = "red")

plot(CO.ts, xlim=c(0,100))
lines(model1$fitted.values, col = "red")
```


### Choose Trend + Seasonal Model
```{r}
CO.lm <- CO.seasonal5.trend
```
Since both models performed similarly in metrics and diagnostics, we have chosen to use the model with the trend and top five identified periods to explain seasonality. It was preferred from the partial F test, and we feel that it is better to choose the simpler model since the two models have such similar performance in metrics and diagnostics.

## Model Residuals
### ACF and PACF
```{r}
# ACF and PACF
acf(CO.ts)
pacf(CO.ts)
```
The ACF is approximately sinusoidal. The PACF does not cut off after a certain number of lags and has some sinusoidal behavior. We predict that there will be autoregressive and moving average terms in the model of the residuals.  
Based on the PACF, potential values for p for the autoregressive portion of the model are 1 and 7.  
Based on the ACF, potential values for q for the moving average portion of the model are 3 and 9.  
Additionally, we are unsure of the stationarity of the data, as we know there is a trend and the ACF appears to have some linear decay at the very beginning. This is a key assumption that must be met, so we will test models using the time series itself (d = 0) and the first differences (d = 1).

### ARIMA Models
We tested the possible combinations of our p, d, and q values.
```{r}
# get residuals
e.ts.CO <-ts(CO.lm$residuals)

# 1, 0, 3
CO.arima103 <- arima(e.ts.CO, order = c(1,0,3), include.mean = FALSE)
summary(CO.arima103)
# AIC = 1439.4

# 1, 0, 9
CO.arima109 <- arima(e.ts.CO, order = c(1,0,9), include.mean = FALSE)
summary(CO.arima109)
# AIC = 1408.98

# 1, 1, 3
CO.arima113 <- arima(e.ts.CO, order = c(1,1,3), include.mean = FALSE)
summary(CO.arima113)
# AIC = 1442.08

# 1, 1, 9
CO.arima119 <- arima(e.ts.CO, order = c(1,1,9), include.mean = FALSE)
summary(CO.arima119)
# AIC = 1432.01

# 7, 0, 3
CO.arima703 <- arima(e.ts.CO, order = c(7,0,3), include.mean = FALSE)
summary(CO.arima703)
# AIC = 1418.29

# 7, 0, 9
CO.arima709 <- arima(e.ts.CO, order = c(7,0,9), include.mean = FALSE)
summary(CO.arima709)
# AIC = 1377.67

# 7, 1, 3
CO.arima713 <- arima(e.ts.CO, order = c(7,1,3), include.mean = FALSE)
summary(CO.arima713)
# AIC = 1429.61

# 7, 1, 9
CO.arima719 <- arima(e.ts.CO, order = c(7,1,9), include.mean = FALSE)
summary(CO.arima719)
# AIC = 1392.05
```
Based on AIC, the two best models are the ARIMA(7,0,9) and ARIMA(7,1,9) models.

After testing the possible values of p, d, and q that we identified, we also used the auto.arima function to generate another model.
```{r}
# auto.arima model
CO.residuals.auto <- auto.arima(e.ts.CO)

# summary
summary(CO.residuals.auto)
```
The ARIMA model is a 5,0,2 model. This means that there are autoregressive and moving average terms, where p = 5 and q = 2. Also, d = 0, meaning that the first differences were not needed to ensure stationarity, which is a key assumption to modeling the residuals. The AIC for this model is 1413.39.

### Diagnostics
```{r}
tsdiag(CO.arima709, gof.lag = 20)
tsdiag(CO.arima719, gof.lag = 20)
tsdiag(CO.residuals.auto, gof.lag = 20)
```
The model that performs the best in diagnostics is the ARIMA(7,1,9) model. It has the most points with high p-values in the Ljung-Box statistic plot. Both of the other models see their p-values decrease sooner as the lag increases.

### Choose Model for Residuals
```{r}
CO.residuals <- CO.arima719
```

## Final Model
The final model includes a trend and seasonality, using 5 different periods (384, 192, 128, 96, and 76.8 days). The ARIMA model of the residuals is a 7,1,9 model.

### create the final model
```{r}

```


## Diagnostics
The residuals of the trend and seasonality linear model appear relatively normal, excluding the lower tail, from the QQ plot; however, problems remain with the residuals vs. fitted plot, indicating lack of fit. In the future, models should address problems with the normality in the lower tail and the lack of fit.
The diagnostic plot for the model of the residuals does not have problems. The p-value for the Ljung-Box stastic is above the dashed line for all 20 lags, meaning the model is adequate for at least up to 20 lags. Future work could focus on increasing the number of lags for which the model is considered adequate.


# Univariate Time Series for NO2
## Visualize the Data
```{r}
# create time series
NO.ts <- ts(air.train$NO2.GT.)

# visualize raw data
plot(NO.ts)
```
There appears to be an increasing trend, especially after about t = 150. There is also the potential for seasonality, but this fluctuation could be random.

```{r}
# periodogram
pg.NO <- spec.pgram(NO.ts,spans=9,demean=T,log='no')
```
Based on the periodogram, there are multiple possibilities for the period of a seasonal component. There are fewer spikes than the periodogram for CO. There are still spikes, though, each of which indicate a possible frequency/period to explain the seasonality of the data. We predict that if the seasonality is significant in the model, it will be based on a complex wave.

## Seasonality
### Finding Potential Periods
```{r}
# sort the frequencies based on influence
sorted.spec.NO <- sort(pg.NO$spec, decreasing = T, index.return=T)

# convert to periods
sorted.omegas.NO <- pg.NO$freq[sorted.spec.NO$ix]
sorted.Ts.NO <- 1/pg.NO$freq[sorted.spec.NO$ix]

plot(sorted.Ts.NO, xlim = c(1,20))

# the cutoff for influential
NO.pg.cutoff <- 15

# the top periods
print('top periods')
sorted.Ts.NO[1:NO.pg.cutoff]

# top frequencies
## to double check that this makes sense based on periodogram
print('top frequencies')
sorted.omegas.NO[1:NO.pg.cutoff]

# visual
NO.pg.box <- boxplot(sorted.Ts.NO[1:NO.pg.cutoff], main="Period Boxplot")

# the average influential period
print('mean of top periods')
NO.pg.box.mean <- NO.pg.box$stats[3]
print(NO.pg.box.mean)
```
We found the frequencies of the largest spikes in the periodogram graph and converted them to periods by taking 1/the frequency. The 15 largest spikes corresponded to periods ranging from 6.62 to 384 days. We considered these our "top" choices for periods to explain seasonality.  
The average influential period can be interpreted as on average, the seasons have a period of 48 days.

### Create Models with Potential Periods
We created models with the top 3, 5, 10, and 15 periods, to determine which was best to explain the seasonality of the data. We also created a model that used a period that was the average of several other periods that were close together.
```{r}
# assign top periods to variables
NO.p1 <- sorted.Ts.NO[1]
NO.p2 <- sorted.Ts.NO[2]
NO.p3 <- sorted.Ts.NO[3]
NO.p4 <- sorted.Ts.NO[4]
NO.p5 <- sorted.Ts.NO[5]
NO.p6 <- sorted.Ts.NO[6]
NO.p7 <- sorted.Ts.NO[7]
NO.p8 <- sorted.Ts.NO[8]
NO.p9 <- sorted.Ts.NO[9]
NO.p10 <- sorted.Ts.NO[10]
NO.p11 <- sorted.Ts.NO[11]
NO.p12 <- sorted.Ts.NO[12]
NO.p13 <- sorted.Ts.NO[13]
NO.p14 <- sorted.Ts.NO[14]
NO.p15 <- sorted.Ts.NO[15]

NO.pavg1 <- mean(c(sorted.Ts.NO[8],sorted.Ts.NO[9], sorted.Ts.NO[10], sorted.Ts.NO[11]))
NO.pavg2 <- mean(c(sorted.Ts.NO[12],sorted.Ts.NO[13], sorted.Ts.NO[14], sorted.Ts.NO[15]))

# create time variable
time.NO<-c(1:length(NO.ts))
```

Model with Top 3 Periods
```{r}
# model
NO.lm.top3 <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3)) 

# model summary
summary(NO.lm.top3)
```

Model with Top 5 Periods
```{r}
# model
NO.lm.top5 <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5)) 

# model summary
summary(NO.lm.top5)
```

Model with Top 10 Periods
```{r}
# model
NO.lm.top10 <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) + 
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.p8) + 
                         cos(2*pi*time.NO/NO.p8) +
                         sin(2*pi*time.NO/NO.p9) + 
                         cos(2*pi*time.NO/NO.p9) +
                         sin(2*pi*time.NO/NO.p10) + 
                         cos(2*pi*time.NO/NO.p10)) 

# model summary
summary(NO.lm.top10)
```

Model with Top 15 Periods
```{r}
# model
NO.lm.top15 <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) + 
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.p8) + 
                         cos(2*pi*time.NO/NO.p8) +
                         sin(2*pi*time.NO/NO.p9) + 
                         cos(2*pi*time.NO/NO.p9) +
                         sin(2*pi*time.NO/NO.p10) + 
                         cos(2*pi*time.NO/NO.p10) +
                         sin(2*pi*time.NO/NO.p11) + 
                         cos(2*pi*time.NO/NO.p11) + 
                         sin(2*pi*time.NO/NO.p12) + 
                         cos(2*pi*time.NO/NO.p12) +
                         sin(2*pi*time.NO/NO.p13) + 
                         cos(2*pi*time.NO/NO.p13) +
                         sin(2*pi*time.NO/NO.p14) + 
                         cos(2*pi*time.NO/NO.p14) +
                         sin(2*pi*time.NO/NO.p15) + 
                         cos(2*pi*time.NO/NO.p15)) 

# model summary
summary(NO.lm.top15)
```

Model with Averaged Periods 
```{r}
NO.lm.combined <- lm(NO.ts ~ sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) +
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.pavg1) + 
                         cos(2*pi*time.NO/NO.pavg1) +
                         sin(2*pi*time.NO/NO.pavg2) + 
                         cos(2*pi*time.NO/NO.pavg2)) 

# model summary
summary(NO.lm.combined)
```

### Model Comparison
```{r}
anova(NO.lm.top3, NO.lm.top5)
# small p-value suggests that the larger model performs better than the smaller model 

anova(NO.lm.top5, NO.lm.top10)
# small p-value suggests that the larger model performs better than the smaller model 

anova(NO.lm.top10, NO.lm.top15)
# large p-value suggests that the smaller model performs better 

anova(NO.lm.top10, NO.lm.combined)
# large p-value suggests that the smaller model performs better 
```
The partial F tests suggest that the model that includes the top 10 periods is better than the models with 3, 5, and 15 periods. The partial F test also suggests that the model with the averaged periods is better than the model with the top 10 periods. Both of these models will be further compared through metrics and diagnostics later, after we determine if there is a significant trend to be included.

```{r}
# visualize
plot(NO.ts)
lines(NO.lm.top10$fitted.values, col = "red")
lines(NO.lm.combined$fitted.values, col = "blue")

legend(0, 350, legend = c("Top 10 Periods", "Combined Periods"), col = c("red", "blue"), lwd = 1)
```
Upon visual inspection, both models get the basic shape of the time series correct, though the model with the averaged periods has smaller fluctuations that may better match the actual data. 

## Trend
### Model Time Series Based on Time
```{r}
# trend model
NO.lm.trend <- lm(NO.ts ~ time.NO)

# summary analysis
summary(NO.lm.trend)
```
The p-value is significant at the 0.05 level, which means that the trend is significant. 

### Visualize Trend
```{r}
plot(time.NO, NO.ts, type = "l")
abline(NO.lm.trend, col = "red")
```

### Model Comparison: Trend and Seasonality Together
```{r}
# model with trend + averaged periods
NO.seasonalavg.trend <- lm(NO.ts ~ time.NO + sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) +
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.pavg1) + 
                         cos(2*pi*time.NO/NO.pavg1) +
                         sin(2*pi*time.NO/NO.pavg2) + 
                         cos(2*pi*time.NO/NO.pavg2))

# model with trend + 10 periods
NO.seasonal10.trend <- lm(NO.ts ~ time.NO + sin(2*pi*time.NO/NO.p1) + 
                         cos(2*pi*time.NO/NO.p1) +
                         sin(2*pi*time.NO/NO.p2) + 
                         cos(2*pi*time.NO/NO.p2) +
                         sin(2*pi*time.NO/NO.p3) + 
                         cos(2*pi*time.NO/NO.p3) +
                         sin(2*pi*time.NO/NO.p4) + 
                         cos(2*pi*time.NO/NO.p4) +
                         sin(2*pi*time.NO/NO.p5) + 
                         cos(2*pi*time.NO/NO.p5) +
                         sin(2*pi*time.NO/NO.p6) + 
                         cos(2*pi*time.NO/NO.p6) +
                         sin(2*pi*time.NO/NO.p7) + 
                         cos(2*pi*time.NO/NO.p7) +
                         sin(2*pi*time.NO/NO.p8) + 
                         cos(2*pi*time.NO/NO.p8) +
                         sin(2*pi*time.NO/NO.p9) + 
                         cos(2*pi*time.NO/NO.p9) +
                         sin(2*pi*time.NO/NO.p10) + 
                         cos(2*pi*time.NO/NO.p10))
```

#### Visual Comparison
```{r}
plot(NO.ts)
lines(NO.seasonalavg.trend$fitted.values, col = "blue")
lines(NO.seasonal10.trend$fitted.values, col = "red")
```
Based on visual inspection, the model with the averaged periods appears to better capture the fluctuations of the data, though both models match the general trend of the data.

#### Comparison Using Metrics
```{r}
# Adjusted R^2: model with trend and averaged periods
summary(NO.seasonalavg.trend)$adj.r.squared

# AIC: model with trend and averaged periods
AIC(NO.seasonalavg.trend)

# Adjusted R^2: model with trend and 10 periods
summary(NO.seasonal10.trend)$adj.r.squared

# AIC: model with trend and 10 periods
AIC(NO.seasonal10.trend)
```
The adjusted R^2 of the model that includes the trend and averaged periods is 0.5622. The adjusted R^2 of the model that includes the trend and 10 periods is 0.5621. Based on adjusted R^2, the models are essentially equivalent.  
The AIC of the model with averaged periods is 3821.945. The AIC of the model with 10 periods is 3823.9. Based on AIC, we would choose the model with averaged periods, because it has the smaller AIC value. The values are very similar, though.

#### Diagnostics
**Residuals vs. Fitted**
```{r}
plot(NO.seasonalavg.trend, which = 1)
plot(NO.seasonal10.trend, which = 1)
```
The mean of the residuals is approximately 0 and has fairly even spread above and below the x-axis for both models.

\pagebreak
**QQ Plot**
```{r}
plot(NO.seasonalavg.trend, which = 2)
plot(NO.seasonal10.trend, which = 2)
```
The residuals are approximately normal for both models, though the tails show some deviation from normality. The model with averaged periods appears to have slightly more normal residuals at the tails.

\pagebreak
**Scale-Location Plot**
```{r}
plot(NO.seasonalavg.trend, which = 3)
plot(NO.seasonal10.trend, which = 3)
```
While the mean is not centered at 0 for either plot, there appears to be relatively good spread in both plots.

\pagebreak
**Residuals vs. Leverage**
```{r}
plot(NO.seasonalavg.trend, which = 5)
plot(NO.seasonal10.trend, which = 5)
```
Neither plot has any points with Cook's distances greater than 0.5.

\pagebreak
```{r}
# plot Cook's distances
plot(NO.seasonalavg.trend,labels.id = NULL, which = 4)
plot(NO.seasonal10.trend,labels.id = NULL, which = 4)
```
Both models have low Cook's distances for all points.

Both models perform very similarly in the diagnostics.

### Choose Trend + Seasonal Model
```{r}
NO.lm <- NO.seasonalavg.trend
```
We have chosen the model with the trend and averaged periods to explain seasonality, as it performed slightly better than the model with trend and 10 periods in AIC, as well as being preferred after the partial F test. Since the diagnostics for both models were similar, AIC and the partial F test were used to make the decision.

## Model Residuals
### ACF and PACF
```{r}
# ACF and PACF
acf(NO.ts)
pacf(NO.ts)
```
The ACF has somewhat sinusoidal behavior and is significant for all 25 lags in view on the plot. The first few lags could be seen as having linear decay, suggesting that the time series may not be stationary and the first differences may need to be taken to meet the stationarity assumption. Values for d of 0 and 1 will be tested in potential models.  
The PACF does not cut off after a certain number of lags and also has somewhat sinusoidal behavior. We predict that there will be autoregressive and moving average terms in the model of the residuals.  
Based on the PACF, potential values for p for the autoregressive portion of the model are 3 and 7.  
Based on the ACF, potential values for q for the moving average portion of the model are 4 and 11. 

### ARIMA Models
We tested the possible combinations of our p and q values.
```{r}
# get residuals
e.ts.NO <-ts(NO.lm$residuals)

# 3, 0, 4
NO.arima304 <- arima(e.ts.NO, order = c(3,0,4), include.mean = FALSE)
summary(NO.arima304)
# NaNs produced

# 3, 0, 11
NO.arima3011 <- arima(e.ts.NO, order = c(3,0,11), include.mean = FALSE)
summary(NO.arima3011)
# NaNs produced

# 3, 1, 4
NO.arima314 <- arima(e.ts.NO, order = c(3,1,4), include.mean = FALSE)
summary(NO.arima314)
# AIC = 3708.73

# 3, 1, 11
NO.arima3111 <- arima(e.ts.NO, order = c(3,1,11), include.mean = FALSE)
summary(NO.arima3111)
# AIC = 3699.85

# 7, 0, 4
NO.arima704 <- arima(e.ts.NO, order = c(7,0,4), include.mean = FALSE)
summary(NO.arima704)
# NaNs produced

# 7, 0, 11
NO.arima7011 <- arima(e.ts.NO, order = c(7,0,11), include.mean = FALSE)
summary(NO.arima7011)
# AIC = 3669.33

# 7, 1, 4
NO.arima714 <- arima(e.ts.NO, order = c(7,1,4), include.mean = FALSE)
summary(NO.arima714)
# NaNs produced

# 7, 1, 11
NO.arima7111 <- arima(e.ts.NO, order = c(7,1,11), include.mean = FALSE)
summary(NO.arima7111)
# NaNs produced
```
Based on AIC, the two best models are the ARIMA(7,0,11) and ARIMA(3,1,11) models.

After testing the possible values of p and q that we identified, we also used the auto.arima function to generate another model.
```{r}
# auto.arima model
NO.residuals.auto <- auto.arima(e.ts.NO, approximation =FALSE)

# summary
summary(NO.residuals.auto)
```
The ARIMA model is a 1,0,0 model. This means that there are autoregressive terms, where p = 1, but no moving average terms, since q = 0. Also, d = 0, meaning that the time series is stationary, which is a key assumption to modeling the residuals. The AIC for this model is 3725.46.

### Diagnostics
```{r}
tsdiag(NO.arima7011, gof.lag = 20)
tsdiag(NO.arima3111, gof.lag = 20)
tsdiag(NO.residuals.auto, gof.lag = 20)
```
The model that performs the best in diagnostics is the ARIMA(3,1,11) model. It has the most points with high p-values in the Ljung-Box statistic plot. The ARIMA(7,0,11) model performed similarly on the diagnostic, but the ARIMA(3,1,11) model was slightly better. Both models performed significantly better than the auto-generated model, meaning that that model was adequate up to fewer lags.

### Choose Model for Residuals
```{r}
NO.residuals <- NO.arima3111
```
This model has the lowest AIC and also the best diagnostic plot of the models considered.

## Final Model
The final model includes a trend and seasonality, using 9 different periods, two of which are the averages of 4 similar periods each. The ARIMA model of the residuals is a 3,1,11 model.

### Create the final Model
```{r}

```

## Diagnostics
The residuals of the trend and seasonality linear model appear relatively normal, from the QQ plot. There are no points with high Cook's distances. The residuals vs. fitted and scale-location plots have minor problems; they could have more even spread and a mean even closer to zero, but overall they look good. In the future, another model could be identified that performs even better in regard to these diagnostics.  
In the diagnostic plot for the model of the residuals, the p-value for the Ljung-Box stastic is above the dashed line for 15 of the 20 lags, meaning that the model of the residuals is adequate for up to 15 lags. A future model could work to extend the model adequacy to all 20 lags of the plot.










#### Multivariate Time Series
## Seasonality and Trends
We used the same linear models for the seasonality and trends that we discovered in our analysis of the univariate time series for CO and NO2. See above for how we modeled seasonal components and trends to produce these linear models.  
The ARMA models for the residuals of both time series use d = 0, meaning that both meet the stationarity assumption without taking the first differences.

## Autoregressive and Moving Average Terms
In our univariate models, the residuals for CO were modeled with an ARIMA(7,1,9) model and the residuals for NO2 were modeled with an ARIMA(3,1,11) model. There were autoregressive and moving average terms in both cases. Since there were different p and q values used to model the residuals for each time series, it is difficult to use the ACF and PACF to determine values for p and q for the multivariate model. Because of this, we will test models with p from 1 to xx and q from 0 to xx to determine the best VARMA model.

## Models
```{r}
# all residuals together
allResiduals <- data.frame(CO.lm$residuals, NO.lm$residuals)

# Build VARMA models
AICmatrix <- matrix(NA, 4, 12)
for(p in 1:4){
  for(q in 0:11){
    varma.model <- VARMACpp(allResiduals, p=p, q=q, include.mean=F)
    AICmatrix[p,q+1] <- varma.model$aic
  }
}
```

## Model Comparison
### Metrics
```{r}
# get AIC matrix
AICmatrix
```
The two models with the lowest AIC values are VARMA(p,q) and VARMA(p,q) with an AIC of xx and xx, respectively.

### Diagnostics
```{r}
# model with lowest AIC
varma.model1 <- VARMACpp(allResiduals, p=1, q=0, include.mean=F)

# model with second lowest AIC
varma.model2 <- VARMACpp(allResiduals, p=1, q=0, include.mean=F)

# diagnostics for model1
MTSdiag(varma.model1)

# diagnostics for model2
MTSdiag(varma.model2)
```
(mask the output of tested models whose diagnostics you don't discuss using include = FALSE)

## Final Model
```{r}
varma.model <- VARMACpp(allResiduals, p=1, q=0, include.mean=F)
# change p and q once we have the final model
```

## Diagnostics
what problems remain in the diagnostics of the final model?


# Forecasting
compare univariate and multivariate models visually and based on MSE


# Simulation from Univariate CO Model
```{r}
# CO univariate model = model of data + model of residuals
CO.uni <- CO.lm + CO.residuals # idk that this is the way to do this

# simulate 1 year of daily maximum CO concentrations
### fix this based on actual ar and ma terms
CO.uni.sim <- arima.sim(n = 365*1, list(ar = c(CO.uni$coef[1], CO.uni$coef[2]), ma = c(CO.uni$coef[3])), sd = sqrt(CO.uni$sigma2))

# make time series
CO.uni.sim.ts <- ts(CO.uni.sim)
```

## Visualization
```{r}
# plot simulated values with observations
plot(CO.ts, main = "Simulation from Univariate CO Model", col = "black")
lines(CO.uni.sim.ts, col = "red")
legend(0.6, 0.57, legend = c("Time Series Data", "Simulation"), col = c("black", "red"))
```
how does simulation compare visually with time series data?

## Trend
```{r}
# linear model for simulation
CO.sim.lm <- lm()
```
compare coefficient estimates

## Seasonality
```{r}
# compare periodogram of observations and periodogram of simulation
par(mfrow = c(1,2))
pg.CO
pg.CO.uni.sim <- spec.pgram(CO.uni.sim.ts, spans = 9, demean = T, log = 'no')
par(mfrow = c(1,1))
```
compare periodograms (get periods?)

## Mean and Variance
```{r}
# mean of observations
mean(CO.ts)

# mean of simulation
mean(CO.uni.sim.ts)

# variance of observations
var(CO.ts)

# variance of simulation
var(CO.uni.sim.ts)
```
compare mean and variance

## Auto-Correlation
```{r}
# ACF of observations
acf(CO.ts)

# ACF of simulation
acf(CO.uni.sim.ts)
```
compare ACF

```{r}
# PACF of observations
pacf(CO.ts)

# PACF of simulation
pacf(CO.uni.sim.ts)
```
compare PACF

## Cross-Correlation
```{r}
# cross-correlation of observations
cor(air.train$CO.GT.)

# cross-correlation of simulation
cor(CO.uni.sim)
```
compare cross-correlation


# Simulation from Univariate NO2 Model
## Visualization
## Trend
## Seasonality
## Mean and Variance
## Auto-Correlation
## Cross-Correlation


# Simulation from Multivariate Model
## Visualization
## Trend
## Seasonality
## Mean and Variance
## Auto-Correlation
## Cross-Correlation

#### Bonus 

# Create forecasts
Forecast for CO 
```{r}
E_Y.pred.CO <- predict(CO.lm, newdata=air.test$CO.GT.)
e_t.pred.CO <- forecast(CO.residuals, h=7)
CO.forecast <- E_Y.pred.CO + e_t.pred.CO$mean
```

Forecast for NO2 
```{r}
E_Y.pred.NO <- predict(NO.lm, newdata=air.test$CO.GT.)
e_t.pred.NO <- forecast(NO.residuals, h=7)
NO.forecast <- E_Y.pred.NO + e_t.pred.NO$mean
```

Forecast for CO and NO2 
```{r}
CO.NO.forecast <- VARMApred(CO.NO.residuals, h=7)

e_t.pred.CO.multi <- CO.NO.forecast$pred[,1]
e_t.pred.CO.lower <- CO.NO.forecast$pred[,1] - 1.96*CO.NO.forecast$se.err[,1]
e_t.pred.CO.upper <- CO.NO.forecast$pred[,1] + 1.96*CO.NO.forecast$se.err[,1]
CO.multi.forecast<- E_Y.pred.CO + e_t.pred.CO.multi

e_t.pred.NO.multi <- CO.NO.forecast$pred[,2]
e_t.pred.NO.lower <- CO.NO.forecast$pred[,2] - 1.96*CO.NO.forecast$se.err[,2]
e_t.pred.NO.upper <- CO.NO.forecast$pred[,2] + 1.96*CO.NO.forecast$se.err[,2]
NO.multi.forecast <- E_Y.pred.NO + e_t.pred.NO.multi

```

# MSE 

```{r}
#CO MSE
mean((CO.forecast-air.test$CO.GT.)^2)

#NO MSE
mean((NO.forecast-air.test$NO2.GT.)^2)

#CO and NO MSE
mean((CO.multi.forecast-air.test$CO.GT.)^2)
mean((NO.multi.forecast-air.test$NO2.GT.)^2)
```


# Visually 
```{r}
#CO Uni plot 
# Plot actual values and predicted values
plot(ts(air.test$CO.GT.),type='o',ylim=c(25,60))
lines(ts(CO.forecast),col='red',type='o')
lines(1:6, E_Y.pred.CO + e_t.pred.CO$lower[,2], col = "red", lty = "dashed")
lines(1:6, E_Y.pred.CO + e_t.pred.CO$upper[,2], col = "red", lty = "dashed")
legend(1,60, legend = c("Actual", "Predicted"), lwd = 2, col = c("black", "red")) 

#NO Uni Plot 
plot(ts(air.test$NO.GT.),type='o',ylim=c(25,60))
lines(ts(NO.forecast),col='red',type='o')
lines(1:6, E_Y.pred.NO + e_t.pred.NO$lower[,2], col = "red", lty = "dashed")
lines(1:6, E_Y.pred.NO + e_t.pred.NO$upper[,2], col = "red", lty = "dashed")
legend(1,60, legend = c("Actual", "Predicted"), lwd = 2, col = c("black", "red")) 

#Multi Plot 
par(mfrow=c(1,2))
# CO forecasts
plot(ts(air.test$CO.GT.),type='o',ylim=c(20,65))
lines(ts(CO.multi.forecast),col='red',type='o')
lines(1:6, E_Y.pred.CO + e_t.pred.CO.lower, col = "red", lty = "dashed")
lines(1:6, E_Y.pred.CO + e_t.pred.CO.upper, col = "red", lty = "dashed")
legend(1,60, legend = c("Actual", "Predicted"), lwd = 2, col = c("black", "red"))

#NO forecasts
plot(ts(air.test$NO2.GT.),type='o',ylim=c(40,85))
lines(ts(NO.multi.forecast),col='red',type='o')
lines(1:6, E_Y.pred.NO + e_t.pred.NO.lower, col = "red", lty = "dashed")
lines(1:6, E_Y.pred.NO + e_t.pred.NO.upper, col = "red", lty = "dashed")
legend(1,80, legend = c("Actual", "Predicted"), lwd = 2, col = c("black", "red"))

```


